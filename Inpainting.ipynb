{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\nInference notebook for [Hotel-ID starter - classification - traning](https://www.kaggle.com/code/michaln/hotel-id-starter-classification-traning)\n\n","metadata":{"id":"DAY5rHgTm7e8","papermill":{"duration":0.024896,"end_time":"2022-03-24T14:00:54.588459","exception":false,"start_time":"2022-03-24T14:00:54.563563","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Readme\nEr staan nu bij input allemaal arcmargin folders, en checkpoint folders met checkpoints files erin. Het irritante is dat Input een read-only file-system is, en je niet alle checkpoint files makkelijk in 1 folder kan stoppen. Verder als je een model traint (in train notebook), vind ik geen makkelijkere manier dan om de checkpoint file te downloaden en manueel hier te uploaden. Als je een makkelijker manier vindt, laat maar weten.\n\n# Let op\nAls je een model traint op bv 512x512 images, moet je IMG_SIZE hier op 2 plekken naar 512 zetten. Je moet verder onderin (bij model_array) naar je getrainde checkpoint verwijzen. Verder als je een ander model hebt gebruikt moet je overal waar efficientnet_b0 staat, dit vervangen door jouw model.","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"executionInfo":{"elapsed":16271,"status":"ok","timestamp":1619310548121,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"alleged-legislation","outputId":"c6541e5f-ffb4-4609-d6c6-39784e6a07b1","papermill":{"duration":0.036572,"end_time":"2022-03-24T14:00:54.649254","exception":false,"start_time":"2022-03-24T14:00:54.612682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:59:44.102957Z","iopub.execute_input":"2022-05-30T16:59:44.103263Z","iopub.status.idle":"2022-05-30T16:59:44.107988Z","shell.execute_reply.started":"2022-05-30T16:59:44.103229Z","shell.execute_reply":"2022-05-30T16:59:44.107271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"cZoSOL9Qm-Yr","papermill":{"duration":0.023644,"end_time":"2022-03-24T14:00:54.696898","exception":false,"start_time":"2022-03-24T14:00:54.673254","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport os\nimport math","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","executionInfo":{"elapsed":14459,"status":"ok","timestamp":1619310548121,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"expired-matter","papermill":{"duration":0.030271,"end_time":"2022-03-24T14:00:54.751131","exception":false,"start_time":"2022-03-24T14:00:54.72086","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:59:55.276027Z","iopub.execute_input":"2022-05-30T16:59:55.27655Z","iopub.status.idle":"2022-05-30T16:59:55.28075Z","shell.execute_reply.started":"2022-05-30T16:59:55.276511Z","shell.execute_reply":"2022-05-30T16:59:55.279687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.utils import class_weight\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm\n\nimport scipy\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n#import plotly.express as px\nimport plotly.graph_objects as go","metadata":{"executionInfo":{"elapsed":16003,"status":"ok","timestamp":1619310550014,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"extreme-problem","papermill":{"duration":3.220402,"end_time":"2022-03-24T14:00:57.995239","exception":false,"start_time":"2022-03-24T14:00:54.774837","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:59:50.666827Z","iopub.execute_input":"2022-05-30T16:59:50.667079Z","iopub.status.idle":"2022-05-30T16:59:50.671739Z","shell.execute_reply.started":"2022-05-30T16:59:50.66705Z","shell.execute_reply":"2022-05-30T16:59:50.671023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport timm\nfrom timm.optim import Lookahead, RAdam","metadata":{"executionInfo":{"elapsed":19672,"status":"ok","timestamp":1619310554099,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"angry-domain","papermill":{"duration":2.727834,"end_time":"2022-03-24T14:01:00.766951","exception":false,"start_time":"2022-03-24T14:00:58.039117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:59:59.377356Z","iopub.execute_input":"2022-05-30T16:59:59.377619Z","iopub.status.idle":"2022-05-30T16:59:59.381918Z","shell.execute_reply.started":"2022-05-30T16:59:59.377589Z","shell.execute_reply":"2022-05-30T16:59:59.381135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global","metadata":{"id":"0B00pe7mnBTj","papermill":{"duration":0.023573,"end_time":"2022-03-24T14:01:00.814976","exception":false,"start_time":"2022-03-24T14:01:00.791403","status":"completed"},"tags":[]}},{"cell_type":"code","source":"SEED = 42\nIMG_SIZE = 256\n\nPROJECT_FOLDER = \"../input/hotel-id-to-combat-human-trafficking-2022-fgvc9/\"\nTRAIN_DATA_FOLDER = \"../input/hotelid-2022-train-images-256x256/images/\"\nTEST_DATA_FOLDER = PROJECT_FOLDER + \"test_images/\"","metadata":{"executionInfo":{"elapsed":589,"status":"ok","timestamp":1619310979015,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"contained-brief","papermill":{"duration":0.03175,"end_time":"2022-03-24T14:01:00.871686","exception":false,"start_time":"2022-03-24T14:01:00.839936","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T17:00:01.122588Z","iopub.execute_input":"2022-05-30T17:00:01.123267Z","iopub.status.idle":"2022-05-30T17:00:01.12705Z","shell.execute_reply.started":"2022-05-30T17:00:01.123228Z","shell.execute_reply":"2022-05-30T17:00:01.126388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(PROJECT_FOLDER))","metadata":{"executionInfo":{"elapsed":879,"status":"ok","timestamp":1619310979515,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"PZvmFng7ctO3","outputId":"dce0cc91-8e70-4acc-a0b8-6763ffffd5ca","papermill":{"duration":0.031651,"end_time":"2022-03-24T14:01:00.927239","exception":false,"start_time":"2022-03-24T14:01:00.895588","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T17:00:03.318986Z","iopub.execute_input":"2022-05-30T17:00:03.319274Z","iopub.status.idle":"2022-05-30T17:00:03.345142Z","shell.execute_reply.started":"2022-05-30T17:00:03.319233Z","shell.execute_reply":"2022-05-30T17:00:03.344335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"executionInfo":{"elapsed":600,"status":"ok","timestamp":1619310981653,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"eastern-content","papermill":{"duration":0.032105,"end_time":"2022-03-24T14:01:01.031949","exception":false,"start_time":"2022-03-24T14:01:00.999844","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T17:00:05.022134Z","iopub.execute_input":"2022-05-30T17:00:05.023107Z","iopub.status.idle":"2022-05-30T17:00:05.028762Z","shell.execute_reply.started":"2022-05-30T17:00:05.023053Z","shell.execute_reply":"2022-05-30T17:00:05.027654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n[17:23]\nJa 4 nog, ik wil nog proberen boven 0.65 te komen, maar heb aan 3 g# Dataset and transformations","metadata":{"id":"xaJKvvuKnW4k","papermill":{"duration":0.023988,"end_time":"2022-03-24T14:01:01.080488","exception":false,"start_time":"2022-03-24T14:01:01.0565","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import albumentations as A\nimport albumentations.pytorch as APT\nimport cv2 \n\nIMG_SIZE = 256\n\nbase_transform = A.Compose([\n    A.ToFloat(),\n    APT.transforms.ToTensorV2(),\n])\n\n\ntest_tta_transforms = {\n    \"base\": A.Compose([A.ToFloat(),  APT.transforms.ToTensorV2(),]),\n    \"crop1\": A.Compose([A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6,1.0), p=1), A.ToFloat(),  APT.transforms.ToTensorV2(),]),\n    \"crop2\": A.Compose([A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6,1.0), p=1),  APT.transforms.ToTensorV2(),]),\n    \"crop3\": A.Compose([A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6,1.0), p=1),  APT.transforms.ToTensorV2(),]),\n    \"crop4\": A.Compose([A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6,1.0), p=1),  APT.transforms.ToTensorV2(),]),\n    \"crop5\": A.Compose([A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6,1.0), p=1),  APT.transforms.ToTensorV2(),]),\n    \"h_flip\": A.Compose([A.ToFloat(), A.HorizontalFlip(p=1), APT.transforms.ToTensorV2(),]),\n    \"v_flip\": A.Compose([A.ToFloat(), A.VerticalFlip(p=1), APT.transforms.ToTensorV2(),]),\n    \"rotate+90\": A.Compose([A.ToFloat(), A.Rotate(limit=90, p=1), APT.transforms.ToTensorV2(),]),\n    \"rotate-90\": A.Compose([A.ToFloat(), A.Rotate(limit=-90, p=1), APT.transforms.ToTensorV2(),]),\n#     \"rand_bright\": A.Compose([A.ToFloat(), A.RandomBrightness(p=1), APT.transforms.ToTensor(),]),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:07.664324Z","iopub.execute_input":"2022-05-30T17:00:07.66458Z","iopub.status.idle":"2022-05-30T17:00:07.677018Z","shell.execute_reply.started":"2022-05-30T17:00:07.66455Z","shell.execute_reply":"2022-05-30T17:00:07.676266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_image(img):\n    w, h, c = np.shape(img)\n    if w > h:\n        pad = int((w - h) / 2)\n        img = cv2.copyMakeBorder(img, 0, 0, pad, pad, cv2.BORDER_CONSTANT, value=0)\n    else:\n        pad = int((h - w) / 2)\n        img = cv2.copyMakeBorder(img, pad, pad, 0, 0, cv2.BORDER_CONSTANT, value=0)\n        \n    return img\n\n\ndef open_and_preprocess_image(image_path):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = pad_image(img)\n    return cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n\n\nclass HotelImageDataset:\n    def __init__(self, data, transform=None, data_folder=\"train_images/\"):\n        self.data = data\n        self.data_folder = data_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        record = self.data.iloc[idx]\n        image_path = self.data_folder + record[\"image_id\"]\n        \n        if \"test\" in self.data_folder:\n            image = np.array(open_and_preprocess_image(image_path)).astype(np.uint8)\n        else:\n            image = np.array(pil_image.open(image_path)).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image)\n        \n        return {\n            \"image\" : transformed[\"image\"],\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:10.397912Z","iopub.execute_input":"2022-05-30T17:00:10.398169Z","iopub.status.idle":"2022-05-30T17:00:10.408522Z","shell.execute_reply.started":"2022-05-30T17:00:10.39814Z","shell.execute_reply":"2022-05-30T17:00:10.407883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inpaint Model for 1 image input","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\ndef extract_mask(img): \n    \n#Outcommented code used to test if the operation worked and visualize what happend\n\n# img = cv2.imread(\"../input/hotel-id-to-combat-human-trafficking-2022-fgvc9/test_images/abc.jpg\", cv2.IMREAD_COLOR) \n# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# mask = cv2.imread(\"../input/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_masks/00000.png\", cv2.IMREAD_COLOR)\n# mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n# plt.imshow(img)\n# plt.imshow(mask)\n\n#find out colour of mask:\n#print(mask.shape)\n#print((mask[:,:,0]==0).all())\n\n#check where the image has a certain colour\nmask_index_picture = np.logical_and(np.logical_and(np.where(img[:,:,0] == 255), np.where(img[:,:,1] == 0),np.where(img[:,:,2] == 0 )\n#in future: do a K-neighbour check to make sure it is a certain size\n\n#extract mask\nart_mask = np.zeros(img.shape)\nart_mask[mask_index_picture] = [255, 0, 0]\n\nreturn art_mask\n\ndef list2nparray(lst, dtype=None):\n    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n    if isinstance(lst, np.ndarray):\n        return lst\n    assert isinstance(lst, (list, tuple)), \"bad type: {}\".format(type(lst))\n    assert lst, \"attempt to convert empty list to np array\"\n    if isinstance(lst[0], np.ndarray):\n        dim1 = lst[0].shape\n        assert all(i.shape == dim1 for i in lst)\n        if dtype is None:\n            dtype = lst[0].dtype\n            assert all(i.dtype == dtype for i in lst), \"bad dtype: {} {}\".format(\n                dtype, set(i.dtype for i in lst)\n            )\n    elif isinstance(lst[0], (int, float, complex, np.number)):\n        return np.array(lst, dtype=dtype)\n    else:\n        dim1 = list2nparray(lst[0])\n        if dtype is None:\n            dtype = dim1.dtype\n        dim1 = dim1.shape\n    shape = [len(lst)] + list(dim1)\n    rst = np.empty(shape, dtype=dtype)\n    for idx, i in enumerate(lst):\n        rst[idx] = i\n    return rst\n\n\ndef get_img_list(path):\n    if Path(path).is_file():\n        return [Path(path)]\n    else:\n        return (\n            sorted(list(Path(path).glob(\"*.png\")))\n            + sorted(list(Path(path).glob(\"*.jpg\")))\n            + sorted(list(Path(path).glob(\"*.jpeg\")))\n        )\n\n\ndef gen_miss(img, mask, output):\n\n    imgs = get_img_list(img)\n    masks = get_img_list(mask) #masks here is a path to the masks image\n    print(\"Total i-mages:\", len(imgs), len(masks))\n\n    out = Path(output)\n    out.mkdir(parents=True, exist_ok=True)\n\n    for i, (img, mask) in enumerate(zip(imgs, masks)):\n        path = out.joinpath(\"miss_%04d.png\" % (i + 1))\n        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n        mask = cv2.imread(str(mask), 0)\n        mask = cv2.resize(mask, img.shape[:2][::-1]) #here mask is resized to shape defined beforehand\n        mask = mask[..., np.newaxis]\n        mask *= 255\n        miss = img * (mask > 127) + 255 * (mask <= 127)\n        plt.imshow(miss)\n        plt.show()\n        cv2.imwrite(str(path), miss)\n\n\ndef merge_imgs(dirs, output, row=1, gap=2, res=512):\n\n    image_list = [get_img_list(path) for path in dirs]\n    img_count = [len(image) for image in image_list]\n    print(\"Total images:\", img_count)\n    assert min(img_count) > 0, \"Please check the path of empty folder.\"\n\n    output_dir = Path(output)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    n_img = len(dirs)\n    row = row\n    column = (n_img - 1) // row + 1\n    print(\"Row:\", row)\n    print(\"Column:\", column)\n\n    for i, unit in tqdm(enumerate(zip(*image_list))):\n        name = output_dir.joinpath(\"merge_%04d.png\" % i)\n        merge = (\n            np.ones(\n                [res * row + (row + 1) * gap, res * column + (column + 1) * gap, 3],\n                np.uint8,\n            )\n            * 255\n        )\n        for j, img in enumerate(unit):\n            r = j // column\n            c = j - r * column\n            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n            if img.shape[:2] != (res, res):\n                img = cv2.resize(img, (res, res))\n            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n            merge[start_h : start_h + res, start_w : start_w + res] = img\n        cv2.imwrite(str(name), merge)\ndef resize_like(x, target, mode=\"bilinear\"):\n    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n\ndef get_norm(name, out_channels):\n    if name == 'batch':\n        norm = nn.BatchNorm2d(out_channels)\n    elif name == 'instance':\n        norm = nn.InstanceNorm2d(out_channels)\n    else:\n        norm = None\n    return norm\n\n\ndef get_activation(name):\n    if name == 'relu':\n        activation = nn.ReLU()\n    elif name == 'elu':\n        activation == nn.ELU()\n    elif name == 'leaky_relu':\n        activation = nn.LeakyReLU(negative_slope=0.2)\n    elif name == 'tanh':\n        activation = nn.Tanh()\n    elif name == 'sigmoid':\n        activation = nn.Sigmoid()\n    else:\n        activation = None\n    return activation\n\n\nclass Conv2dSame(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super().__init__()\n\n        padding = self.conv_same_pad(kernel_size, stride)\n        if type(padding) is not tuple:\n            self.conv = nn.Conv2d(\n                in_channels, out_channels, kernel_size, stride, padding)\n        else:\n            self.conv = nn.Sequential(\n                nn.ConstantPad2d(padding*2, 0),\n                nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n            )\n\n    def conv_same_pad(self, ksize, stride):\n        if (ksize - stride) % 2 == 0:\n            return (ksize - stride) // 2\n        else:\n            left = (ksize - stride) // 2\n            right = left + 1\n            return left, right\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass ConvTranspose2dSame(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super().__init__()\n\n        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n        self.trans_conv = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size, stride,\n            padding, output_padding)\n\n    def deconv_same_pad(self, ksize, stride):\n        pad = (ksize - stride + 1) // 2\n        outpad = 2 * pad + stride - ksize\n        return pad, outpad\n\n    def forward(self, x):\n        return self.trans_conv(x)\n\n\nclass UpBlock(nn.Module):\n\n    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n        super().__init__()\n\n        self.mode = mode\n        if mode == 'deconv':\n            self.up = ConvTranspose2dSame(\n                channel, channel, kernel_size, stride=scale)\n        else:\n            def upsample(x):\n                return F.interpolate(x, scale_factor=scale, mode=mode)\n            self.up = upsample\n\n    def forward(self, x):\n        return self.up(x)\n\n\nclass EncodeBlock(nn.Module):\n\n    def __init__(\n            self, in_channels, out_channels, kernel_size, stride,\n            normalization=None, activation=None):\n        super().__init__()\n\n        self.c_in = in_channels\n        self.c_out = out_channels\n\n        layers = []\n        layers.append(\n            Conv2dSame(self.c_in, self.c_out, kernel_size, stride))\n        if normalization:\n            layers.append(get_norm(normalization, self.c_out))\n        if activation:\n            layers.append(get_activation(activation))\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass DecodeBlock(nn.Module):\n\n    def __init__(\n            self, c_from_up, c_from_down, c_out, mode='nearest',\n            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n        super().__init__()\n\n        self.c_from_up = c_from_up\n        self.c_from_down = c_from_down\n        self.c_in = c_from_up + c_from_down\n        self.c_out = c_out\n\n        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n\n        layers = []\n        layers.append(\n            Conv2dSame(self.c_in, self.c_out, kernel_size, stride=1))\n        if normalization:\n            layers.append(get_norm(normalization, self.c_out))\n        if activation:\n            layers.append(get_activation(activation))\n        self.decode = nn.Sequential(*layers)\n\n    def forward(self, x, concat=None):\n        out = self.up(x)\n        if self.c_from_down > 0:\n            out = torch.cat([out, concat], dim=1)\n        out = self.decode(out)\n        return out\n\n\nclass BlendBlock(nn.Module):\n\n    def __init__(\n            self, c_in, c_out, ksize_mid=3, norm='batch', act='leaky_relu'):\n        super().__init__()\n        c_mid = max(c_in // 2, 32)\n        self.blend = nn.Sequential(\n            Conv2dSame(c_in, c_mid, 1, 1),\n            get_norm(norm, c_mid),\n            get_activation(act),\n            Conv2dSame(c_mid, c_out, ksize_mid, 1),\n            get_norm(norm, c_out),\n            get_activation(act),\n            Conv2dSame(c_out, c_out, 1, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.blend(x)\n\n\nclass FusionBlock(nn.Module):\n    def __init__(self, c_feat, c_alpha=1):\n        super().__init__()\n        c_img = 3\n        self.map2img = nn.Sequential(\n            Conv2dSame(c_feat, c_img, 1, 1),\n            nn.Sigmoid())\n        self.blend = BlendBlock(c_img*2, c_alpha)\n\n    def forward(self, img_miss, feat_de):\n        img_miss = resize_like(img_miss, feat_de)\n        raw = self.map2img(feat_de)\n        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n        result = alpha * raw + (1 - alpha) * img_miss\n        return result, alpha, raw\n\n\nclass DFNet(nn.Module):\n    def __init__(\n            self, c_img=3, c_mask=1, c_alpha=3,\n            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n            blend_layers=[0, 1, 2, 3, 4, 5]):\n        super().__init__()\n\n        c_init = c_img + c_mask\n\n        self.n_en = len(en_ksize)\n        self.n_de = len(de_ksize)\n        assert self.n_en == self.n_de, (\n            'The number layer of Encoder and Decoder must be equal.')\n        assert self.n_en >= 1, (\n            'The number layer of Encoder and Decoder must be greater than 1.')\n\n        assert 0 in blend_layers, 'Layer 0 must be blended.'\n\n        self.en = []\n        c_in = c_init\n        self.en.append(\n            EncodeBlock(c_in, 64, en_ksize[0], 2, None, None))\n        for k_en in en_ksize[1:]:\n            c_in = self.en[-1].c_out\n            c_out = min(c_in*2, 512)\n            self.en.append(EncodeBlock(\n                c_in, c_out, k_en, stride=2,\n                normalization=norm, activation=act_en))\n\n        # register parameters\n        for i, en in enumerate(self.en):\n            self.__setattr__('en_{}'.format(i), en)\n\n        self.de = []\n        self.fuse = []\n        for i, k_de in enumerate(de_ksize):\n\n            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n            c_out = c_from_down = self.en[-i-1].c_in\n            layer_idx = self.n_de - i - 1\n\n            self.de.append(DecodeBlock(\n                c_from_up, c_from_down, c_out, mode, k_de, scale=2,\n                normalization=norm, activation=act_de))\n            if layer_idx in blend_layers:\n                self.fuse.append(FusionBlock(c_out, c_alpha))\n            else:\n                self.fuse.append(None)\n\n        # register parameters\n        for i, de in enumerate(self.de[::-1]):\n            self.__setattr__('de_{}'.format(i), de)\n        for i, fuse in enumerate(self.fuse[::-1]):\n            if fuse:\n                self.__setattr__('fuse_{}'.format(i), fuse)\n\n    def forward(self, img_miss, mask):\n\n        out = torch.cat([img_miss, mask], dim=1)\n\n        out_en = [out]\n        for encode in self.en:\n            out = encode(out)\n            out_en.append(out)\n\n        results = []\n        alphas = []\n        raws = []\n        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n            out = decode(out, out_en[-i-2])\n            if fuse:\n                result, alpha, raw = fuse(img_miss, out)\n                results.append(result)\n                alphas.append(alpha)\n                raws.append(raw)\n\n        return results[::-1], alphas[::-1], raws[::-1]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:14.356918Z","iopub.execute_input":"2022-05-30T17:00:14.357536Z","iopub.status.idle":"2022-05-30T17:00:14.424956Z","shell.execute_reply.started":"2022-05-30T17:00:14.357497Z","shell.execute_reply":"2022-05-30T17:00:14.424008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nfrom itertools import islice\nfrom multiprocessing.pool import ThreadPool as Pool\nimport os\nfrom pathlib import Path\n\nimport argparse\nimport cv2\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nclass Tester:\n    def __init__(self, model_path, input_size, batch_size):\n        self.model_path = model_path\n        self._input_size = input_size\n        self.batch_size = batch_size\n        self.init_model(model_path)\n\n    @property\n    def input_size(self):\n        if self._input_size > 0:\n            return (self._input_size, self._input_size)\n        elif \"celeba\" in self.model_path:\n            return (256, 256)\n        else:\n            return (512, 512)\n\n    def init_model(self, path):\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")\n            print(\"Using gpu.\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(\"Using cpu.\")\n\n        self.model = DFNet().to(self.device)\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint)\n        self.model.eval()\n\n        print(\"Model %s loaded.\" % path)\n\n    def get_name(self, path):\n        return \".\".join(Path(path).name.split(\".\")[:-1])\n\n    def results_path(self, output, img_path, mask_path, prefix=\"result\"):\n        img_name = self.get_name(img_path)\n        mask_name = self.get_name(mask_path)\n        return {\n            \"result_path\": self.sub_dir(\"result\").joinpath(\n                \"result-{}-{}.png\".format(img_name, mask_name)\n            ),\n            \"raw_path\": self.sub_dir(\"raw\").joinpath(\n                \"raw-{}-{}.png\".format(img_name, mask_name)\n            ),\n            \"alpha_path\": self.sub_dir(\"alpha\").joinpath(\n                \"alpha-{}-{}.png\".format(img_name, mask_name)\n            ),\n        }\n\n    def inpaint_instance(self, img, mask):\n        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n        img = img.view(1, *img.shape)\n        mask = mask.view(1, 1, *mask.shape)\n        return self.inpaint_batch(img, mask).squeeze()\n\n    def inpaint_batch(self, imgs, masks):\n        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n        imgs = np.transpose(imgs, [0, 3, 1, 2])\n        masks = np.transpose(masks, [0, 3, 1, 2])\n\n        imgs = torch.from_numpy(imgs).to(self.device)\n        masks = torch.from_numpy(masks).to(self.device)\n        imgs = imgs.float().div(255)\n        masks = masks.float().div(255)\n        imgs_miss = imgs * masks\n        results = self.model(imgs_miss, masks)\n        if type(results) is list:\n            results = results[0]\n        results = results.mul(255).byte().data.cpu().numpy()\n        results = np.transpose(results, [0, 2, 3, 1])\n        return results\n\n    def _process_file(self, output, img_path, mask_path):\n        item = {\"img_path\": img_path, \"mask_path\": mask_path}\n        item.update(self.results_path(output, img_path, mask_path))\n        self.path_pair.append(item)\n\n    def process_single_file(self, output, img_path, mask_path):\n        self.path_pair = []\n        self._process_file(output, img_path, mask_path)\n\n    def process_dir(self, output, img_dir, mask_dir):\n        img_dir = Path(img_dir)\n        mask_dir = Path(mask_dir)\n        imgs_path = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\")))\n        masks_path = sorted(list(mask_dir.glob(\"*.jpg\")) + list(mask_dir.glob(\"*.png\")))\n\n        n_img = len(imgs_path)\n        n_mask = len(masks_path)\n        n_pair = min(n_img, n_mask)\n\n        self.path_pair = []\n        for i in range(n_pair):\n            img_path = imgs_path[i % n_img]\n            mask_path = masks_path[i % n_mask]\n            self._process_file(output, img_path, mask_path)\n\n    def get_process(self, input_size):\n        def process(pair):\n            img = cv2.imread(str(pair[\"img_path\"]), cv2.IMREAD_COLOR)\n            mask = cv2.imread(str(pair[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n            print(mask)\n            if input_size:\n                img = cv2.resize(img, input_size)\n                mask = cv2.resize(mask, input_size)\n            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n            mask = np.ascontiguousarray(np.expand_dims(mask, 0)).astype(np.uint8)\n            print(mask)\n\n            pair[\"img\"] = img\n            pair[\"mask\"] = mask\n            return pair\n\n        return process\n\n    def _file_batch(self):\n        pool = Pool()\n\n        n_pair = len(self.path_pair)\n        n_batch = (n_pair - 1) // self.batch_size + 1\n\n        for i in range(n_batch):\n            _buffer = defaultdict(list)\n            start = i * self.batch_size\n            stop = start + self.batch_size\n            process = self.get_process(self.input_size)\n            batch = pool.imap_unordered(process, islice(self.path_pair, start, stop))\n            for instance in batch:\n                for k, v in instance.items():\n                    _buffer[k].append(v)\n            yield _buffer\n\n    def batch_generator(self):\n        generator = self._file_batch\n\n        for _buffer in generator():\n            for key in _buffer:\n                if key in [\"img\", \"mask\"]:\n                    _buffer[key] = list2nparray(_buffer[key])\n            yield _buffer\n\n    def to_numpy(self, tensor):\n        tensor = tensor.mul(255).byte().data.cpu().numpy()\n        tensor = np.transpose(tensor, [0, 2, 3, 1])\n        return tensor\n\n    def process_batch(self, batch, output):\n        imgs = torch.from_numpy(batch[\"img\"]).to(self.device)\n        masks = torch.from_numpy(batch[\"mask\"]).to(self.device)\n        imgs = imgs.float().div(255)\n        masks = masks.float().div(255)\n        imgs_miss = imgs * masks\n\n        result, alpha, raw = self.model(imgs_miss, masks)\n        result, alpha, raw = result[0], alpha[0], raw[0]\n        result = imgs * masks + result * (1 - masks)\n\n        result = self.to_numpy(result)\n        alpha = self.to_numpy(alpha)\n        raw = self.to_numpy(raw)\n\n        for i in range(result.shape[0]):\n            cv2.imwrite(str(batch[\"result_path\"][i]), result[i])\n            cv2.imwrite(str(batch[\"raw_path\"][i]), raw[i])\n            cv2.imwrite(str(batch[\"alpha_path\"][i]), alpha[i])\n\n    @property\n    def root(self):\n        return Path(self.output)\n\n    def sub_dir(self, sub):\n        return self.root.joinpath(sub)\n\n    def prepare_folders(self, folders):\n        for folder in folders:\n            Path(folder).mkdir(parents=True, exist_ok=True)\n\n    def inpaint(self, output, img, mask, merge_result=False):\n\n        self.output = output\n        self.prepare_folders(\n            [self.sub_dir(\"result\"), self.sub_dir(\"alpha\"), self.sub_dir(\"raw\")]\n        )\n\n        if os.path.isfile(img) and os.path.isfile(mask):\n            if img.endswith((\".png\", \".jpg\", \".jpeg\")):\n                self.process_single_file(output, img, mask)\n                _type = \"file\"\n            else:\n                raise NotImplementedError()\n        elif os.path.isdir(img) and os.path.isdir(mask):\n            self.process_dir(output, img, mask)\n            _type = \"dir\"\n        else:\n            print(\"Img: \", img)\n            print(\"Mask: \", mask)\n            raise NotImplementedError(\"img and mask should be both file or directory.\")\n\n        print(\"# Inpainting...\")\n        print(\"Input size:\", self.input_size)\n        for batch in self.batch_generator():\n            self.process_batch(batch, output)\n        print(\"Inpainting finished.\")\n\n        if merge_result:\n            miss = self.sub_dir(\"miss\")\n            merge = self.sub_dir(\"merge\")\n\n            print(\"# Preparing input images...\")\n            gen_miss(img, mask, miss)\n            print(\"# Merging...\")\n            merge_imgs(\n                [\n                    miss,\n                    self.sub_dir(\"raw\"),\n                    self.sub_dir(\"alpha\"),\n                    self.sub_dir(\"result\"),\n                    img,\n                ],\n                merge,\n                res=self.input_size[0],\n            )\n            print(\"Merging finished.\")\n\n\ntester = Tester('../input/df-model-places2/model_places2.pth', 512, 16)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:23.636217Z","iopub.execute_input":"2022-05-30T17:00:23.636486Z","iopub.status.idle":"2022-05-30T17:00:24.090051Z","shell.execute_reply.started":"2022-05-30T17:00:23.636448Z","shell.execute_reply":"2022-05-30T17:00:24.089291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example image\nim = '../input/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/100055/000003766.jpg'\nms = '../input/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_masks/00002.png'\nout = ''\ntester.inpaint('./test.png', im, ms, True)\n# python test.py --model model/model_celeba.pth --img samples/celeba/img --mask samples/celeba/mask --output output/celeba --merge","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:29.040959Z","iopub.execute_input":"2022-05-30T17:00:29.041341Z","iopub.status.idle":"2022-05-30T17:00:30.871447Z","shell.execute_reply.started":"2022-05-30T17:00:29.041306Z","shell.execute_reply":"2022-05-30T17:00:30.870754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir('test.png/result'))\n\nimport imageio\nim = imageio.imread('./test.png/result/result-000003766-00002.png')\n\nplt.figure()\nplt.imshow(im)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:00:33.61018Z","iopub.execute_input":"2022-05-30T17:00:33.610429Z","iopub.status.idle":"2022-05-30T17:00:33.844162Z","shell.execute_reply.started":"2022-05-30T17:00:33.610399Z","shell.execute_reply":"2022-05-30T17:00:33.843467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"NMDM4PwPnced","papermill":{"duration":0.023902,"end_time":"2022-03-24T14:01:01.962307","exception":false,"start_time":"2022-03-24T14:01:01.938405","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# source: https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n\n        return output\n\nclass HotelIdModel(nn.Module):\n    def __init__(self, out_features, embed_size=256, backbone_name=\"densenet161\"):\n        super(HotelIdModel, self).__init__()\n\n        self.embed_size = embed_size\n        self.backbone = timm.create_model(backbone_name, pretrained=False)\n        in_features = self.backbone.get_classifier().in_features\n\n        fc_name, _ = list(self.backbone.named_modules())[-1]\n        if fc_name == 'classifier':\n            self.backbone.classifier = nn.Identity()\n        elif fc_name == 'head.fc':\n            self.backbone.head.fc = nn.Identity()\n        elif fc_name == 'head.flatten':\n            self.backbone.head.fc = nn.Identity()\n        elif fc_name == 'fc':\n            self.backbone.fc = nn.Identity()\n        else:\n            raise Exception(\"unknown classifier layer: \" + fc_name)\n\n        self.arc_face = ArcMarginProduct(self.embed_size, out_features, s=30.0, m=0.50, easy_margin=False)\n\n        self.post = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(in_features, self.embed_size*2), dim=None),\n            nn.BatchNorm1d(self.embed_size*2),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.embed_size*2, self.embed_size)),\n            nn.BatchNorm1d(self.embed_size),\n        )\n\n        print(f\"Model {backbone_name} ArcMarginProduct - Features: {in_features}, Embeds: {self.embed_size}\")\n        \n    def forward(self, input, targets = None):\n        x = self.backbone(input)\n        x = x.view(x.size(0), -1)\n        x = self.post(x)\n        \n        if targets is not None:\n            logits = self.arc_face(x, targets)\n            return logits\n        \n        return x","metadata":{"papermill":{"duration":0.032166,"end_time":"2022-03-24T14:01:02.018479","exception":false,"start_time":"2022-03-24T14:01:01.986313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:01:41.258646Z","iopub.execute_input":"2022-05-30T16:01:41.258915Z","iopub.status.idle":"2022-05-30T16:01:41.278761Z","shell.execute_reply.started":"2022-05-30T16:01:41.258884Z","shell.execute_reply":"2022-05-30T16:01:41.277671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbeddingNet(nn.Module):\n    def __init__(self, n_classes=100, embed_size=64, backbone_name=\"efficientnet_b0\"):\n        super(EmbeddingNet, self).__init__()\n        \n        self.embed_size = embed_size\n        self.backbone = timm.create_model(backbone_name, pretrained=False)\n        in_features = self.backbone.get_classifier().in_features\n\n        fc_name, _ = list(self.backbone.named_modules())[-1]\n        if fc_name == 'classifier':\n            self.backbone.classifier = nn.Identity()\n        elif fc_name == 'head.fc':\n            self.backbone.head.fc = nn.Identity()\n        elif fc_name == 'fc':\n            self.backbone.fc = nn.Identity()\n        else:\n            raise Exception(\"unknown classifier layer: \" + fc_name)\n        \n        self.post = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(in_features, self.embed_size*2), dim=None),\n            nn.BatchNorm1d(self.embed_size*2),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(self.embed_size*2, self.embed_size)),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.BatchNorm1d(self.embed_size),\n            nn.Dropout(0.2),\n            nn.Linear(self.embed_size, n_classes),\n        )\n        \n        print(f\"Model {backbone_name} EmbeddingNet - Features: {in_features}, Embeds: {self.embed_size}\")\n        \n    def embed_and_classify(self, x):\n        x = self.forward(x)\n        return x, self.classifier(x)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = x.view(x.size(0), -1)\n        x = self.post(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-30T16:01:49.723046Z","iopub.execute_input":"2022-05-30T16:01:49.723423Z","iopub.status.idle":"2022-05-30T16:01:49.734188Z","shell.execute_reply.started":"2022-05-30T16:01:49.723388Z","shell.execute_reply":"2022-05-30T16:01:49.733523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model helper functions","metadata":{"id":"YMZYKhUSneMY","papermill":{"duration":0.024153,"end_time":"2022-03-24T14:01:02.067537","exception":false,"start_time":"2022-03-24T14:01:02.043384","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef get_embeds(loader, model, bar_desc=\"Generating embeds\"):\n    outputs_all = []\n    \n    model.eval()\n    with torch.no_grad():\n        t = tqdm(loader, desc=bar_desc)\n        for i, sample in enumerate(t):\n            input = sample['image'].to(args.device)\n            output = model(input)\n            outputs_all.extend(output.detach().cpu().numpy())\n#             outputs_all.extend(output.detach().cpu().numpy().astype(np.float16))\n            \n            \n    return outputs_all","metadata":{"execution":{"iopub.status.busy":"2022-05-30T16:01:58.988163Z","iopub.execute_input":"2022-05-30T16:01:58.988828Z","iopub.status.idle":"2022-05-30T16:01:58.995354Z","shell.execute_reply.started":"2022-05-30T16:01:58.988788Z","shell.execute_reply":"2022-05-30T16:01:58.994288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_distances(input, base_embeds, model_array):\n    distances = None\n    for i, model in enumerate(model_array):\n        output = model(input)\n        output = output.detach().cpu().numpy()\n#         output = output.detach().cpu().numpy().astype(np.float16)\n        model_base_embeds = base_embeds[i]\n        output_distances = cosine_similarity(output, model_base_embeds)\n        \n        if distances is None:\n            distances = output_distances\n        else:\n            distances = distances * output_distances\n            \n    return distances\n    \n\ndef predict(loader, base_df, base_embeds, model_array, n_matches=5, bar_desc=\"Generating embeds\"):\n    preds = []\n    with torch.no_grad():\n        t = tqdm(loader, desc=bar_desc)\n        for i, sample in enumerate(t):\n            input = sample['image'].to(args.device)\n            distances = get_distances(input, base_embeds, model_array)\n            \n            for j in range(len(distances)):\n                tmp_df = base_df.copy()\n                tmp_df[\"distance\"] = distances[j]\n                tmp_df = tmp_df.sort_values(by=[\"distance\", \"hotel_id\"], ascending=False).reset_index(drop=True)\n                preds.extend([tmp_df[\"hotel_id\"].unique()[:n_matches]])\n\n    return preds\n\ndef find_closest_match(args, test_loader, base_loader, model_array, n_matches=5):\n    base_embeds = {}\n    for i, model in enumerate(model_array):\n        base_embeds[i] = get_embeds(base_loader, model, \"Generating embeds for train\")\n    \n    preds = predict(test_loader, base_loader.dataset.data, base_embeds, model_array, n_matches, f\"Generating predictions\")\n        \n    return preds\n\ndef transform(original_image):\n    tensor_to_numpy = originalimage[0][\"image\"].numpy() \n    switched_channels = np.swapaxes(tensor_to_numpy, 0, 2)\n    return switched_channels\n\ndef get_mask(original image):\n    img = transform(original_image)\n    \n    \n    ","metadata":{"papermill":{"duration":0.034692,"end_time":"2022-03-24T14:01:02.127372","exception":false,"start_time":"2022-03-24T14:01:02.09268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:02:13.866369Z","iopub.execute_input":"2022-05-30T16:02:13.866643Z","iopub.status.idle":"2022-05-30T16:02:13.878806Z","shell.execute_reply.started":"2022-05-30T16:02:13.866612Z","shell.execute_reply":"2022-05-30T16:02:13.87784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{"id":"AwShW1wXniD6","papermill":{"duration":0.023807,"end_time":"2022-03-24T14:01:02.175822","exception":false,"start_time":"2022-03-24T14:01:02.152015","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#test_df = pd.DataFrame(data={\"image_id\": os.listdir(TEST_DATA_FOLDER), \"hotel_id\": \"\"}).sort_values(by=\"image_id\")\ndata_df = pd.read_csv(\"../input/hotelid-2022-train-images-256x256/train.csv\")\nsample_submission_df = pd.read_csv(PROJECT_FOLDER + \"sample_submission.csv\")\ntest_df = pd.DataFrame(data={\"image_id\": os.listdir(TEST_DATA_FOLDER), \"hotel_id\": \"\"}).sort_values(by=\"image_id\")","metadata":{"executionInfo":{"elapsed":3742,"status":"ok","timestamp":1619311036476,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"discrete-right","outputId":"c21ed589-3139-4919-b5d5-07bcf6f1df15","papermill":{"duration":0.103381,"end_time":"2022-03-24T14:01:02.453866","exception":false,"start_time":"2022-03-24T14:01:02.350485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:02:17.737742Z","iopub.execute_input":"2022-05-30T16:02:17.738407Z","iopub.status.idle":"2022-05-30T16:02:17.793927Z","shell.execute_reply.started":"2022-05-30T16:02:17.738372Z","shell.execute_reply":"2022-05-30T16:02:17.793187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code hotel_id mapping created in training notebook by encoding hotel_ids\nhotel_id_code_df = pd.read_csv('../input/hotelcodemapping-org/hotel_id_code_mapping.csv')\nhotel_id_code_map = hotel_id_code_df.set_index('hotel_id_code').to_dict()[\"hotel_id\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T16:02:20.685755Z","iopub.execute_input":"2022-05-30T16:02:20.686291Z","iopub.status.idle":"2022-05-30T16:02:20.722507Z","shell.execute_reply.started":"2022-05-30T16:02:20.686251Z","shell.execute_reply":"2022-05-30T16:02:20.721571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare model","metadata":{"id":"5JPdD2bpnniP","papermill":{"duration":0.023835,"end_time":"2022-03-24T14:01:02.502786","exception":false,"start_time":"2022-03-24T14:01:02.478951","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_model(model_type, backbone_name, embed_size, checkpoint_path, args):\n    if model_type == 'arcmargin':\n        model = HotelIdModel(3116, embed_size, backbone_name)\n    else:\n        model = EmbeddingNet(3116, embed_size, backbone_name)\n        \n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model\"])\n    model = model.to(args.device)\n    \n    return model","metadata":{"papermill":{"duration":0.031082,"end_time":"2022-03-24T14:01:02.557921","exception":false,"start_time":"2022-03-24T14:01:02.526839","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:02:30.991787Z","iopub.execute_input":"2022-05-30T16:02:30.992037Z","iopub.status.idle":"2022-05-30T16:02:30.998315Z","shell.execute_reply.started":"2022-05-30T16:02:30.992008Z","shell.execute_reply":"2022-05-30T16:02:30.99742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    batch_size = 32\n    num_workers = 2\n    n_classes = data_df[\"hotel_id\"].nunique()\n    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    \nseed_everything(seed=SEED)\n\nbase_dataset = HotelImageDataset(data_df, base_transform, data_folder=TRAIN_DATA_FOLDER)\nbase_loader = DataLoader(base_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\ntest_dataset = HotelImageDataset(test_df, test_tta_transforms[\"base\"], data_folder=TEST_DATA_FOLDER)\ntest_dataset_paint = tester.inpaint(transform())\ntest_loader = DataLoader(test_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\nimagetonump = (test_dataset[0]['image'].numpy())\nprint(imagetonump.shape)\n\n#test_dataset1 = HotelImageDataset(test_df, test_tta_transforms[\"base\"], data_folder=TEST_DATA_FOLDER)\n#test_loader1 = DataLoader(test_dataset1, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n#test_dataset2 = HotelImageDataset(test_df, test_tta_transforms[\"h_flip\"], data_folder=TEST_DATA_FOLDER)\n#test_loader2 = DataLoader(test_dataset2, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n#test_dataset3 = HotelImageDataset(test_df, test_tta_transforms[\"crop3\"], data_folder=TEST_DATA_FOLDER)\n#test_loader3 = DataLoader(test_dataset3, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n#test_dataset4 = HotelImageDataset(test_df, test_tta_transforms[\"crop4\"], data_folder=TEST_DATA_FOLDER)\n#test_loader4 = DataLoader(test_dataset4, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n#test_dataset5 = HotelImageDataset(test_df, test_tta_transforms[\"crop5\"], data_folder=TEST_DATA_FOLDER)\n#test_loader5 = DataLoader(test_dataset5, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)","metadata":{"executionInfo":{"elapsed":450,"status":"ok","timestamp":1619311064188,"user":{"displayName":"Jeom Jin-Ho","photoUrl":"","userId":"00155613517919499503"},"user_tz":-120},"id":"appointed-machinery","papermill":{"duration":0.069839,"end_time":"2022-03-24T14:01:02.65177","exception":false,"start_time":"2022-03-24T14:01:02.581931","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:31:08.583924Z","iopub.execute_input":"2022-05-30T16:31:08.58429Z","iopub.status.idle":"2022-05-30T16:31:08.608244Z","shell.execute_reply.started":"2022-05-30T16:31:08.584256Z","shell.execute_reply":"2022-05-30T16:31:08.607393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"timm.list_models(pretrained=True)","metadata":{}},{"cell_type":"code","source":"model_array = [get_model(\"arcmargin\", \n                         \"eca_nfnet_l0\", 4096,\n                         \"../input/cp-ecanfnet-256-ep6/checkpoint-arcmargin-model-eca_nfnet_l0-256x256-4096embeds-3116hotels-6.pt\", \n                         args),\n              ]\n              \n              #get_model(\"arcmargin\", \n                         #\"efficientnet_b2\", 4096,\n                         #\"../input/arcmargin512-effb2-ep9/checkpoint-arcmargin-model-efficientnet_b2-512x512-4096embeds-3116hotels-9.pt\", \n                         #args),\n              #]","metadata":{"papermill":{"duration":5.553999,"end_time":"2022-03-24T14:01:08.229948","exception":false,"start_time":"2022-03-24T14:01:02.675949","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-30T16:31:39.190643Z","iopub.execute_input":"2022-05-30T16:31:39.191342Z","iopub.status.idle":"2022-05-30T16:31:40.415271Z","shell.execute_reply.started":"2022-05-30T16:31:39.191287Z","shell.execute_reply":"2022-05-30T16:31:40.414129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.024227,"end_time":"2022-03-24T14:01:08.278753","exception":false,"start_time":"2022-03-24T14:01:08.254526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#%%time\n\n#preds = predict(test_loader, model)\n# replace classes with hotel_id using mapping created in trainig notebook\n#preds = [[hotel_id_code_map[b] for b in a] for a in preds]\n# transform array of hotel_ids into string\n#test_df[\"hotel_id\"] = [str(list(l)).strip(\"[]\").replace(\",\", \"\") for l in preds]\n\n#test_df.to_csv(\"submission.csv\", index=False)\n#test_df.head()\n#%%time\n\nif len(test_df) > 3:\n    preds = find_closest_match(args, test_loader, base_loader, model_array, n_matches = 5)\n    #df_1 = find_closest_match(args, test_loader1, base_loader, model_array, n_matches=5)\n    #print(df_1[:5])\n    #df_2 = find_closest_match(args, test_loader1, base_loader, model_array, n_matches=5)\n    #print(df_2[:5])\n    #df3 = df_1.merge(df_2, on = 'image_id')\n    #print(df3[:5])\n    #df_2 = find_closest_match(args, test_loader1, base_loader, model_array, n_matches=5)\n    #df_2 = find_closest_match(args, test_loader1, base_loader, model_array, n_matches=5)\n    #df_2 = find_closest_match(args, test_loader1, base_loader, model_array, n_matches=5)\n    #preds2, distances2 = find_closest_match(args, test_loader2, base_loader, model_array, n_matches=5)\n    #preds3, distances3 = find_closest_match(args, test_loader3, base_loader, model_array, n_matches=5)\n    #preds4, distances4 = find_closest_match(args, test_loader4, base_loader, model_array, n_matches=5)\n    #preds5, distances5 = find_closest_match(args, test_loader5, base_loader, model_array, n_matches=5)\n    test_df[\"hotel_id\"] = [str(list(l)).strip(\"[]\").replace(\",\", \"\") for l in preds]\n\ntest_df.to_csv(\"submission.csv\", index=False)\ntest_df.head()","metadata":{"papermill":{"duration":0.047156,"end_time":"2022-03-24T14:01:08.414557","exception":false,"start_time":"2022-03-24T14:01:08.367401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-25T13:14:13.004932Z","iopub.execute_input":"2022-05-25T13:14:13.005269Z","iopub.status.idle":"2022-05-25T13:14:13.035816Z","shell.execute_reply.started":"2022-05-25T13:14:13.005231Z","shell.execute_reply":"2022-05-25T13:14:13.035154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds = find_closest_match(args, test_loader, base_loader, model_array, n_matches=5)\n\n#test_df[\"hotel_id_pred\"] = [str(list(l)).strip(\"[]\").replace(\",\", \"\") for l in preds]\n\n#y = np.repeat([test_df[\"hotel_id\"]], repeats=5, axis=0).T\n#preds = np.array(preds)\n\n#acc_top_1 = (preds[:, 0] == test_df[\"hotel_id\"]).mean()\n#acc_top_5 = (preds == y).any(axis=1).mean()\n\n#print(f\"Accuracy: {acc_top_1:0.4f}, top 5 accuracy: {acc_top_5:0.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T13:14:13.037468Z","iopub.execute_input":"2022-05-25T13:14:13.037862Z","iopub.status.idle":"2022-05-25T13:14:13.042906Z","shell.execute_reply.started":"2022-05-25T13:14:13.037826Z","shell.execute_reply":"2022-05-25T13:14:13.042085Z"},"trusted":true},"execution_count":null,"outputs":[]}]}